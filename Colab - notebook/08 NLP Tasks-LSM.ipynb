{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"16nZPPDeONU0sndaNXLp2a-Apxe7FhIcV","timestamp":1699784228359}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Task to the topic Natural Language Processing (NLP)\n","\n","You task is to implement all the steps of NLP algorithm"],"metadata":{"id":"pweh8gHhK9W8"}},{"cell_type":"code","source":["!pip install git+https://github.com/mehalyna/cooltest.git"],"metadata":{"id":"eypDxo5QPYfM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699785649579,"user_tz":-120,"elapsed":10169,"user":{"displayName":"lee scriber","userId":"13285270051983739409"}},"outputId":"b9832778-78ef-4ec1-c5c3-8f7ab4529815"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/mehalyna/cooltest.git\n","  Cloning https://github.com/mehalyna/cooltest.git to /tmp/pip-req-build-96lybief\n","  Running command git clone --filter=blob:none --quiet https://github.com/mehalyna/cooltest.git /tmp/pip-req-build-96lybief\n","  Resolved https://github.com/mehalyna/cooltest.git to commit f4c950440e06f6870bf813c9e2f1b253f1f497ba\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.10/dist-packages (from cooltest==26.18) (1.23.5)\n","Building wheels for collected packages: cooltest\n","  Building wheel for cooltest (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for cooltest: filename=cooltest-26.18-py3-none-any.whl size=5420 sha256=5da1f2d0213b5d6fcb7212c3ea3c29b18968bd50b4c24c843d97702d6252ee17\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-0h9svmlb/wheels/5f/d0/08/46fba8323b078d91da2d05922a680d9728e94d53b453a8dd79\n","Successfully built cooltest\n","Installing collected packages: cooltest\n","Successfully installed cooltest-26.18\n"]}]},{"cell_type":"code","source":["from cooltest.test_cool_1 import *"],"metadata":{"id":"udZTSvO5Pl4q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["1. Segmentation.\n","\n","Break the entire document down into its constituent sentences. You can do this by segmenting the article along with its punctuations like full stops and commas."],"metadata":{"id":"S83RDTVnLOnG"}},{"cell_type":"markdown","source":["Write the body of function ```get_segment``` which would be used for the segmentation of the given ```text```. Function will return the sequence of the segments as the result."],"metadata":{"id":"eEPJ_iSBMzxf"}},{"cell_type":"code","source":["#install all the needed modules\n","import spacy\n","#load core english library\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","@test_get_segment\n","def get_segment(some_text):\n","  nlp = spacy.load(\"en_core_web_sm\")\n","  #take string\n","  doc = nlp(some_text)\n","  sentences = []\n","  #to print sentences\n","  for sentence in doc.sents:\n","    sentences.append(sentence)\n","  return sentences"],"metadata":{"id":"ngrJjj9MMAsR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699795896307,"user_tz":-120,"elapsed":1620,"user":{"displayName":"lee scriber","userId":"13285270051983739409"}},"outputId":"81761034-5e4e-4936-e893-49f0ba135e50"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Segmentaition Passed\n","\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TaGVEppQKown"},"outputs":[],"source":["text = '''Natural Language Processing (NLP) is a part of AI (artificial intelligence) that deals with understanding and processing of human language.\n","In real time, majority of data exists in the unstructured form such us text, videos, images.\n","Mass of data in unstructured category, will be in textual form.\n","To process this textual data's with machine learning algorithms, NLP comes in to play.\n","NLP use cases are Language translation, Speech recognition, Hiring and Recruitment, Chat Bot, Sentimental analysis and so on.'''"]},{"cell_type":"code","source":["segment=get_segment(text)\n","print(*segment, sep=\"\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rihQDEdEHspp","executionInfo":{"status":"ok","timestamp":1699795902519,"user_tz":-120,"elapsed":1018,"user":{"displayName":"lee scriber","userId":"13285270051983739409"}},"outputId":"71fc82f1-ce9e-42b9-e1b2-c86f5511e6e3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Natural Language Processing (NLP) is a part of AI (artificial intelligence) that deals with understanding and processing of human language.\n","\n","In real time, majority of data exists in the unstructured form such us text, videos, images.\n","\n","Mass of data in unstructured category, will be in textual form.\n","\n","To process this textual data's with machine learning algorithms, NLP comes in to play.\n","\n","NLP use cases are Language translation, Speech recognition, Hiring and Recruitment, Chat Bot, Sentimental analysis and so on.\n"]}]},{"cell_type":"markdown","source":["2. Tokenizing\n","\n","Define the body of the function ```get_tokens()``` that would be used for breaking down given sentence into its constituent words (tokens). The function will return the sequence of the tokens as the result."],"metadata":{"id":"D7IcU-K-TXg_"}},{"cell_type":"code","source":["from spacy.tokenizer import Tokenizer\n","from spacy.lang.en import English\n","\n","@test_get_tokens\n","def get_tokens(some_text):\n","  nlp = English()\n","  tokenizer = Tokenizer(nlp.vocab)\n","  tokens_nlp = tokenizer(some_text)\n","  return tokens_nlp"],"metadata":{"id":"0O2I1a9MPjx2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699795909198,"user_tz":-120,"elapsed":683,"user":{"displayName":"lee scriber","userId":"13285270051983739409"}},"outputId":"fab98ef2-3bcc-4990-e950-8b48765d4b1f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenization Passed\n","\n"]}]},{"cell_type":"code","source":["doc = \"Natural Language Processing IS THE best choice for the learning\"\n","tokens = get_tokens(doc)\n","print(len(tokens))\n","print(*tokens, sep=\" | \")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bdby_R7cOrfu","executionInfo":{"status":"ok","timestamp":1699795933382,"user_tz":-120,"elapsed":391,"user":{"displayName":"lee scriber","userId":"13285270051983739409"}},"outputId":"35ef5755-8b96-4814-fc48-c29a392869e2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["10\n","Natural | Language | Processing | IS | THE | best | choice | for | the | learning\n"]}]},{"cell_type":"markdown","source":["3. Removing Stop Words\n","\n","Your next task is getting rid of non-essential words, which add little meaning to the statement ```new_tokens``` and are just there to make your statement sound more cohesive. Words such as *was, in, is, and, the*, are called stop words and can be removed.\n","Write the body of function ```remove_stop_words()``` which will take the parameter sequence with tokens (```some_tokens```) and return as the result the sequence of tokens without non-essential words (*stop words*)"],"metadata":{"id":"Rk_4IHk0VIU0"}},{"cell_type":"code","source":["@test_remove_stop_words\n","def remove_stop_words(tokens):\n","  filtered_sentence =[]\n","\n","  for word in tokens:\n","      if word.is_stop == False:\n","          filtered_sentence.append(word)\n","\n","  return filtered_sentence\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gRRayuxnXCcr","executionInfo":{"status":"ok","timestamp":1699795939581,"user_tz":-120,"elapsed":1624,"user":{"displayName":"lee scriber","userId":"13285270051983739409"}},"outputId":"888a09ba-df51-48e1-a576-d9792606fc87"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Stop Words Removing Passed\n","\n"]}]},{"cell_type":"code","source":["filtered_sentence = remove_stop_words(tokens)\n","print(*filtered_sentence, sep=\" | \")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sjbg_Za0Xnkq","executionInfo":{"status":"ok","timestamp":1699795945972,"user_tz":-120,"elapsed":321,"user":{"displayName":"lee scriber","userId":"13285270051983739409"}},"outputId":"cebc626c-9889-444d-bcb6-2dc963e3de15"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Natural | Language | Processing | best | choice | learning\n"]}]},{"cell_type":"code","source":["# import module for stop words removing\n","from spacy.lang.en.stop_words import STOP_WORDS\n","\n","@test_remove_stop_words\n","def remove_stop_words(tokens):\n","    filtered_sentence = []\n","\n","    for word in tokens:\n","        if word.text.lower() not in STOP_WORDS:\n","            filtered_sentence.append(word.text)\n","\n","    return filtered_sentence\n","\n"],"metadata":{"id":"hEB8Zmk7VHgl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699795951745,"user_tz":-120,"elapsed":792,"user":{"displayName":"lee scriber","userId":"13285270051983739409"}},"outputId":"d1bcd195-53b3-48fb-8d0c-a7a1b15ac20e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Stop Words Removing Passed\n","\n"]}]},{"cell_type":"code","source":["filtered_sentence = remove_stop_words(tokens)\n","print(*filtered_sentence, sep=\" | \")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WS0gl592SabR","executionInfo":{"status":"ok","timestamp":1699795955799,"user_tz":-120,"elapsed":384,"user":{"displayName":"lee scriber","userId":"13285270051983739409"}},"outputId":"2af948c4-1615-4e37-e1a1-f72b7d7e2791"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Natural | Language | Processing | best | choice | learning\n"]}]},{"cell_type":"markdown","source":["4. Stemming and Lemmatization\n","\n","Now you have to obtain the Root Stem of a word. Root Stem gives the new base form of a word that is present in the dictionary and from which the word is derived. You can also identify the base word for different words, - lemma.\n","\n","Write the body ```stemm_lemmatization()``` of function which should return the list of lemmas of given parameter text.\n","\n"],"metadata":{"id":"jvyNsFHIMREe"}},{"cell_type":"code","source":["\n","@test_stemm_lemmatization\n","def stemm_lemmatization(text):\n","  nlp = spacy.load('en_core_web_sm')\n","  doc = nlp(text)\n","  stemm_lemm = []\n","  for token in doc:\n","    stemm_lemm.append(token.lemma_)\n","\n","  return  stemm_lemm"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NZS26-GyaNqS","executionInfo":{"status":"ok","timestamp":1699795991108,"user_tz":-120,"elapsed":1717,"user":{"displayName":"lee scriber","userId":"13285270051983739409"}},"outputId":"d3cb37cd-1868-4542-edfb-3f22970f4199"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Stemming and Lemmatization Passed\n","\n"]}]},{"cell_type":"code","source":["print(stemm_lemmatization(doc))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jxmRvbVPvFXw","executionInfo":{"status":"ok","timestamp":1699796001349,"user_tz":-120,"elapsed":2331,"user":{"displayName":"lee scriber","userId":"13285270051983739409"}},"outputId":"3c4bb2a1-7003-43a2-a495-08a880e01b23"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Natural', 'Language', 'Processing', 'be', 'the', 'good', 'choice', 'for', 'the', 'learning']\n"]}]},{"cell_type":"code","source":["import nltk\n","from nltk.stem import PorterStemmer\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","nltk.download('punkt')\n","nltk.download('wordnet')\n"],"metadata":{"id":"ETXuYBh3NM2r","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699796007864,"user_tz":-120,"elapsed":411,"user":{"displayName":"lee scriber","userId":"13285270051983739409"}},"outputId":"1f0176f9-7e9b-4072-9243-8b2d6f9350fc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":132}]},{"cell_type":"code","source":["def stemm_lemmatization(text):\n","\n","    tokens = word_tokenize(text)\n","    porter_stemmer = PorterStemmer()\n","    wordnet_lemmatizer = WordNetLemmatizer()\n","    lemmatized_words = []\n","    stemmed_words = []\n","    for token in tokens:\n","        stemmed_word = porter_stemmer.stem(str(token).lower())\n","        lemmatized_word = wordnet_lemmatizer.lemmatize(str(token).lower(), pos=\"v\")  # pos='v' для дієслів\n","        stemmed_words.append(stemmed_word)\n","        lemmatized_words.append(lemmatized_word)\n","    return stemmed_words, lemmatized_words\n","\n","text = \"Natural Language Processing IS THE best choice for the learning\"\n","stemmed_words, lemmatized_words = stemm_lemmatization(text)\n","print(\"stemmed_words   \", stemmed_words)\n","print(\"lemmatized_words\", lemmatized_words)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GozeGslgfFsI","executionInfo":{"status":"ok","timestamp":1699796021389,"user_tz":-120,"elapsed":587,"user":{"displayName":"lee scriber","userId":"13285270051983739409"}},"outputId":"b1190e7b-ac48-444a-ae3f-aa7a55cd7049"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["stemmed_words    ['natur', 'languag', 'process', 'is', 'the', 'best', 'choic', 'for', 'the', 'learn']\n","lemmatized_words ['natural', 'language', 'process', 'be', 'the', 'best', 'choice', 'for', 'the', 'learn']\n"]}]},{"cell_type":"markdown","source":["5. Part of Speech Tagging:\n","\n","Now, you have to define the body of function ```part_of()``` that will take the sentence of text as the parameter. The function shoul return the dictionary with words as keys and appropriate tags (parts of speech) as values."],"metadata":{"id":"6OL7n8PbObCF"}},{"cell_type":"code","source":["import spacy\n","@test_part_of\n","def part_of(text):\n","\n","  nlp = spacy.load('en_core_web_sm')\n","  doc = nlp(text)\n","  word_tags = {}\n","  for word in doc:\n","    word_tags[word.text] = word.pos_\n","\n","  return word_tags"],"metadata":{"id":"KBWUv8cpW57l","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699796069065,"user_tz":-120,"elapsed":1685,"user":{"displayName":"lee scriber","userId":"13285270051983739409"}},"outputId":"e61b94e9-26ce-45e3-916a-01eddc01a7a6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Parts of Speech Passed\n","\n"]}]},{"cell_type":"code","source":["text = \"Natural Language Processing IS THE best choice for the learning\"\n","word_tags= part_of(text)\n","print(word_tags)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_rD4EbQurOfe","executionInfo":{"status":"ok","timestamp":1699796074017,"user_tz":-120,"elapsed":802,"user":{"displayName":"lee scriber","userId":"13285270051983739409"}},"outputId":"ecd994dc-6e6f-4a33-83c4-a91cfa1dcacc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'Natural': 'PROPN', 'Language': 'PROPN', 'Processing': 'PROPN', 'IS': 'AUX', 'THE': 'DET', 'best': 'ADJ', 'choice': 'NOUN', 'for': 'ADP', 'the': 'DET', 'learning': 'NOUN'}\n"]}]}]}